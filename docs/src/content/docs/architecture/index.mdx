---
title: Architecture Overview
description: High-level architecture and design principles of Alita Robot.
---

import { Badge, Tabs, TabItem, Card, CardGrid } from '@astrojs/starlight/components';

Alita Robot is a modern Telegram group management bot built with Go and the gotgbot library. This document provides an overview of the architectural decisions, technology stack, and design principles that guide the codebase.

## Technology Stack

<Tabs>
  <TabItem label="Go Runtime">
    | Component | Technology | Purpose |
    |-----------|------------|---------|
    | **Language** | <Badge text="Go 1.25+" variant="note" /> | Core application runtime |
    | **Telegram Library** | <Badge text="gotgbot v2" variant="note" /> | Telegram Bot API wrapper |
    | **Build System** | <Badge text="GoReleaser" variant="default" /> | Multi-platform builds and releases |
  </TabItem>
  <TabItem label="PostgreSQL">
    | Component | Technology | Purpose |
    |-----------|------------|---------|
    | **Database** | <Badge text="PostgreSQL" variant="tip" /> | Persistent data storage |
    | **ORM** | <Badge text="GORM" variant="tip" /> | Object-relational mapping |
    | **Migrations** | <Badge text="Custom Engine" variant="default" /> | Schema versioning with transactional execution |

    The database uses a **surrogate key pattern**: auto-increment `id` as PK with external IDs (`user_id`, `chat_id`) as unique constraints.
  </TabItem>
  <TabItem label="Redis">
    | Component | Technology | Purpose |
    |-----------|------------|---------|
    | **Caching** | <Badge text="Redis" variant="caution" /> | Distributed caching layer |
    | **Client** | <Badge text="gocache" variant="caution" /> | Go cache library with marshaling |
    | **Stampede Protection** | <Badge text="singleflight" variant="default" /> | Prevents thundering herd on cache misses |

    All cache keys are prefixed with `alita:` for namespace isolation. TTLs range from 20 seconds (anonymous admin verification) to 1 hour (language preferences).
  </TabItem>
  <TabItem label="Monitoring">
    | Component | Technology | Purpose |
    |-----------|------------|---------|
    | **Metrics** | <Badge text="Prometheus" variant="success" /> | Metrics and observability |
    | **Tracing** | <Badge text="OpenTelemetry" variant="success" /> | Distributed tracing with OTLP/console exporters |
    | **Health** | <Badge text="HTTP /health" variant="default" /> | Unified health, metrics, pprof on single port |
  </TabItem>
</Tabs>

## Architecture Components

<CardGrid>
  <Card title="Dispatcher" icon="rocket">
    Routes incoming Telegram updates to appropriate handlers. Configurable max goroutines (default: 200) with enhanced error handling and panic recovery.
  </Card>
  <Card title="Module System" icon="puzzle">
    Feature modules in `alita/modules/` follow a consistent pattern: `moduleStruct`, handler methods, `LoadModule` registration. Help module loads last to collect all commands.
  </Card>
  <Card title="Cache Layer" icon="star">
    Redis caching with singleflight stampede protection. Per-type TTLs, automatic invalidation on writes, and traced operations via OpenTelemetry.
  </Card>
  <Card title="Permission System" icon="approve-check">
    Centralized permission validation in `alita/utils/chat_status/`. Checks are cached to reduce Telegram API calls. Supports anonymous admin detection.
  </Card>
  <Card title="Monitoring" icon="seti:config">
    4-tier auto-remediation, activity tracking (per-chat and per-user DAU/WAU/MAU), background stats every 30s, and GC triggers on memory thresholds.
  </Card>
  <Card title="Graceful Shutdown" icon="close">
    Central coordinator with LIFO handler execution. Each handler gets panic recovery. Total timeout: 60 seconds.
  </Card>
</CardGrid>

## Core Design Principles

### 1. Domain Functions Pattern

All database operations are organized by domain in `alita/db/*_db.go` files. Each file provides Get/Add/Update/Delete functions for its domain, with surrogate key pattern (auto-increment `id` as PK, external IDs as unique constraints):

```go
// Example: Direct domain function calls
settings := db.GetChatSettings(chatId)
db.UpdateChatSettings(chatId, newSettings)
```

### 2. Decorator Pattern

Middleware functionality is implemented through decorators in `alita/utils/decorators/`. Common cross-cutting concerns are handled uniformly:

- Permission checking (admin, restrict, delete rights)
- Error handling with panic recovery
- Logging and metrics collection

### 3. Worker Pools

Concurrent processing uses bounded worker pools with panic recovery:

- **Dispatcher**: Limited to 200 max goroutines by default (configurable via `DISPATCHER_MAX_ROUTINES`)
- **Message Pipeline**: Concurrent validation stages
- **Bulk Operations**: Parallel batch processors with generic framework

### 4. Redis Caching

Redis-based caching with stampede protection:

- Distributed cache using Redis for persistence across restarts
- Singleflight pattern prevents thundering herd on cache misses
- Configurable TTLs per data type (30min - 1hr typically)

## Request Flow Diagram

```
                                    +------------------+
                                    |    Telegram      |
                                    |    Bot API       |
                                    +--------+---------+
                                             |
                          +------------------+------------------+
                          |                                     |
                   Webhook Mode                           Polling Mode
                          |                                     |
                          v                                     v
               +----------+----------+              +-----------+-----------+
               |   HTTP Server       |              |      Updater          |
               |   /webhook/{secret} |              |   GetUpdates loop     |
               +----------+----------+              +-----------+-----------+
                          |                                     |
                          +------------------+------------------+
                                             |
                                             v
                                  +----------+----------+
                                  |     Dispatcher      |
                                  | (configurable routines)|
                                  +----------+----------+
                                             |
                         +-------------------+-------------------+
                         |                   |                   |
                         v                   v                   v
                  +------+------+     +------+------+     +------+------+
                  |   Handler   |     |   Handler   |     |   Handler   |
                  |  (Command)  |     | (Callback)  |     |  (Message)  |
                  +------+------+     +------+------+     +------+------+
                         |                   |                   |
                         +-------------------+-------------------+
                                             |
                              +--------------+--------------+
                              |                             |
                              v                             v
                    +---------+----------+        +---------+----------+
                    |   Redis Cache      |        |   PostgreSQL       |
                    |   (cache lookup)    |        |   (via GORM)       |
                    +--------------------+        +--------------------+
```

## Key Subsystems

### Dispatcher

The dispatcher routes incoming Telegram updates to appropriate handlers:

- Configurable max goroutines (default: 200)
- Enhanced error handler with structured logging
- Recovery from panics in any handler

```go
dispatcher := ext.NewDispatcher(&ext.DispatcherOpts{
    Error: func(b *gotgbot.Bot, ctx *ext.Context, err error) ext.DispatcherAction {
        // Error handling with structured logging
        return ext.DispatcherActionNoop
    },
    MaxRoutines: config.AppConfig.DispatcherMaxRoutines,
})
```

### Module System

Each feature module follows a consistent pattern:

1. Define a `moduleStruct` with module name
2. Implement handler functions as methods
3. Register handlers in a `LoadModule` function
4. Module loaded via `alita/main.go` in specific order

### Cache Layer

Redis caching with stampede protection:

- **Cache Keys**: Prefixed with `alita:` for namespace isolation
- **Stampede Protection**: Singleflight prevents concurrent cache rebuilds
- **TTL Management**: Per-type expiration (settings: 30min, language: 1hr)

### Monitoring

Comprehensive monitoring subsystems:

- **Resource Monitor**: Tracks memory and goroutine usage every 5 minutes
- **Activity Monitor**: Automatic group activity tracking with configurable thresholds
- **Background Stats**: Performance metrics collection
- **Auto-Remediation**: GC triggers when memory exceeds thresholds

## Concurrency Model

### Bounded Concurrency

```go
// Dispatcher limits concurrent handler execution
MaxRoutines: 200  // Configurable via DISPATCHER_MAX_ROUTINES

// Worker pools use bounded parallelism
workerPool := NewWorkerPool(config.AppConfig.WorkerPoolSize)
```

### Singleflight for Cache

```go
// Prevents multiple goroutines from rebuilding same cache entry
var cacheGroup singleflight.Group

result, err, _ := cacheGroup.Do(cacheKey, func() (any, error) {
    // Only one goroutine executes this
    return loadFromDatabase()
})
```

### Graceful Shutdown

```go
// Shutdown manager coordinates cleanup in order
shutdownManager := shutdown.NewManager()
shutdownManager.RegisterHandler(func() error {
    // Cleanup monitoring, database, cache
    return nil
})
```

## Error Handling Strategy

Alita uses a 4-layer error handling hierarchy:

:::note[Error Recovery Layers]
The 4-layer hierarchy ensures that no panic or error goes unhandled. Each layer catches what the layer below it might miss, providing defense-in-depth against crashes.
:::

### Layer 1: Dispatcher Level

```go
Error: func(b *gotgbot.Bot, ctx *ext.Context, err error) ext.DispatcherAction {
    defer error_handling.RecoverFromPanic("DispatcherErrorHandler", "Main")
    // Log error with structured fields
    return ext.DispatcherActionNoop
}
```

### Layer 2: Worker Level

Worker pools implement panic recovery:

```go
go func() {
    defer func() {
        if r := recover(); r != nil {
            log.WithField("panic", r).Error("Panic in worker")
        }
    }()
    // Worker logic
}()
```

### Layer 3: Handler Level

Individual handlers use decorators for error handling:

```go
// Permission checks return early on failure
if !chat_status.RequireUserAdmin(b, ctx, nil, user.Id, false) {
    return ext.EndGroups
}
```

### Layer 4: Decorator Level

Command decorators provide permission validation and error context:

```go
// Decorators wrap handlers with cross-cutting concerns
cmdDecorator.MultiCommand(dispatcher, []string{"cmd", "alias"}, handler)
```

## Database Schema Design

The database uses a **surrogate key pattern**:

- **Primary Keys**: Auto-incremented `id` field (internal identifier)
- **Business Keys**: `user_id` and `chat_id` with unique constraints
- **Benefits**:
  - Decouples internal schema from Telegram IDs
  - Stable identifiers if external systems change
  - Better performance for joins and indexing

:::note[Why Surrogate Keys?]
Telegram IDs are external identifiers that the project does not control. Using auto-increment internal IDs as primary keys decouples the schema from Telegram's ID space, provides stable join targets, and allows better index performance on frequently queried columns.
:::

## Next Steps

- [Project Structure](/architecture/project-structure) - Directory layout and key files
- [Request Flow](/architecture/request-flow) - Detailed update processing pipeline
- [Module Pattern](/architecture/module-pattern) - How to add new modules
- [Caching](/architecture/caching) - Redis caching architecture
